---
layout: distill
title: "Lecture 18: Deep generative models (part 2)"
description: Deep generative models.
date: 2019-03-25

lecturers:
  - name: Zhiting Hu
    url: "#"

authors:
  - name: Yifan Sun
    url: "#"  # optional URL to the author's homepage
  - name: 
    url: "#"
  - name: 
    url: "#"
  - name: 
    url: "#"

editors:
  - name: 
    url: "#"  # optional URL to the editor's homepage

---

<!-- Yifan -->

## Learning with constraints
### Integrating Domain Knowledge into Deep Learning:
* Deep learning heavily relies on massive labeled data.
* DL is hard to interpret and does not encode domain knowledge.
* Want to integrate our domain knowledge into the DL algorithm.

### Set Up of Constraint Learning
* Consider a generative model $x\sim p_\theta(x)$ with objective function $\mathcal{L}(\theta)$. 
* Domain knowledge is represented by a constraint function $f_\theta(x)\in\mathbbm{R}$.
  * Higher $f$ values means better knowledge on $x$. 
  
### Direct Objective Optimization and Difficulties
* The way to impose constraint is to maximize $\mathbbm{E}_{p_\theta}[f(x)]$.
* This term could be added as a regularization term, which leads to the optimization problem:
  $\min_\theta \mathcal{L}(\theta) - \alpha\mathbbm{E}_{p_\theta}[f(x)]$. 
* In general,$\mathbbm{E}_{p_\theta}[f]$ is hard to optimize unless $p_\theta$ is a GAN-like implicit generative model or an explicit distribution that can be efficiently reparametrized.
  For other models such as the large set of non-reparametrizable explicit distributions, the gradient $\nabla\mathbbm{E}_{p_\theta}[f_\theta(x)]$ is usually computed with the log-derivative trick and can suffer from high variance. 

### Variational Method
* Instead of optimizing $\mathbbm{E}_{p_\theta}[f_\theta(x)]$, we can introduce a variational distribution $q$ which is encouraged to stay close to $p_\theta(x)$, we consider the following term 
$\mathcal{L}(\theta,q)=KL(q(x)\|p_\theta(x))-\alpha\mathbbm{E}_q[f]$ as proxy of $-\mathbbm{E}_{p_\theta}[f]$.
* The objective now becomes 
$$\min_\theta \mathcal{L}(\theta) + \alpha\mathcal{L}(\theta,q)$$.
* The problem could be solve by EM algorithm. 
  * In E step, one optimize $\mathcal{L}(\theta,q)$ with respect to $q$ and get $$q*(x)=p_\theta(x)\exp{\alpha f(x)} / Z$$ where $Z$ is the normalization term.
  * In M step, one plugs in $q^*$ and optimize w.r.t. $\theta$ with:
    $$\min_\theta KL(q(x)\|p_\theta(x))=\min_\theta -\mathbbm{E}_q[\log p_\theta(x)] + const$$

### Learning the Knowledge Constraint
* In previous formulation, constraint $f(x)$ has to be fully-specified a priori and is fixed throughout the learning. It would be desirable or necessary to enable learnable constraint.
* We would like to let constraint function has learnable components, we write the constraint as $f_\phi(x)$ where $\phi$ are learnable parameters. 
* Learning $\phi$ is not straightforward, if we optimize $\phi$ in M-step, i.e. 
  $$\max_\phi \mathbbm{E}_{x\sim q(x)}[f_\phi(x)]$$, then such objective could be problematic becuase poor state of the generative parameter $\theta$ at the initial stage.
* One can treat learning of constraint as an extrinsic reward motived by Reinforcement Learning, and get a joint learning algorithm for $\theta$ and $\phi$. For details, please refer to Hu et.al. <d-cite key="Hu2018a"></d-cite>.




